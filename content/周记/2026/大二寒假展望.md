总结过去一学期，对具身这个领域有了一些初步的认知，但是没有进行有效的实践，对模型内部细节不是很熟悉，所以我想通过这个寒假对VLA的训练全流程。

模型选择了三个模型尝试上手：
1. $\pi_{0.5}$
2. OpenVLA-oft
3. SmolVLA
其中pi0.5因为4090有点跑不动训练和全量微调，OpenVLA是token化的动作，而SmolVLA是VLM+flow这个目前比较流行的架构，并且非常小，只有450M，而且代码量也较小，方便自行修改，比如说可以尝试把RECAP训练方式迁移到这里。并且基于Lerobot。
缺点是模型大小较小，泛化能力可能一般。

先前想针对微调开始研究，即SFT和RFT，但是感觉直接上手RL难度曲线有点陡峭，所以目前打算先**跑通训练的全流程，熟悉训练pipeline**，包括：
- 模型输入输出格式
- 推理的过程，阅读推理脚本
- 在LIBERO上跑eval，以及在微调后再跑
- 跑通SFT，阅读SFT代码
- 完整跑一次训练
- 阅读并理解模型的代码
- dataloader和数据预处理
- 修改超参，做实验观察超参和模型的关系
- 冻结VLM进行训练观察VLM的作用
远期的目标：
- 掌握Flow，diffusion，RL的理论基础
- 尝试自己搓一个RL的RFT代码，甚至尝试迁移其他论文的RL方法进来

期间做好文本记录，阅读代码的时候可以自行绘制流程图。

同时保持对论文的阅读，目前先打算把许华哲发的[2025年度论文](https://www.xiaohongshu.com/explore/694cfd25000000001e0082e6?xsec_token=ABoVY1ueVfpB79lGkOpT8WMOtoVnO9xdeN4Zht4YT7bI4=&xsec_source=pc_user)读一下，可以挑一些可迁移的尝试迁移到SmolVLA中。

> Opus 4.5列的规划
```
================== Phase 1: 基础搭建（Week 1）==================

目标：能跑起来，建立环境

Day 1-2: 环境搭建
├── LeRobot + SmolVLA 安装
├── LIBERO 仿真环境
├── 验证：import 不报错，能启动仿真
└── 产出：环境搭建笔记（踩坑记录）

Day 3-4: 跑通推理
├── 下载 SmolVLA checkpoint
├── 单条推理测试
├── 在 LIBERO 上跑 eval
└── 产出：理解 input/output 格式，记录下来

Day 5-6: 探索数据
├── 读 LeRobotDataset 格式
├── 可视化几条训练数据
├── 理解 action chunking 在数据层面怎么体现
└── 产出：数据格式笔记 + 可视化脚本

Day 7: 周总结
├── 整理本周笔记
├── 画出目前理解的系统图
└── 列出还不懂的问题

Milestone: 能跑 eval，能解释数据格式

================== Phase 2: SFT 训练（Week 2）==================

目标：跑通训练，理解训练流程

Day 1-2: 跑通 SFT
├── 用默认配置启动训练
├── 配置 wandb
├── 让它跑着，同时读 train.py
└── 产出：训练启动 checklist

Day 3-4: 理解训练代码
├── 读 train.py 主循环
├── 读 modeling_smolvla.py（forward pass）
├── 读 loss 计算部分
└── 产出：训练流程图（手画）

Day 5-6: 评估和分析
├── 用训练好的模型跑 eval
├── 对比训练前后的成功率
├── 分析 loss 曲线
└── 产出：第一份实验报告

Day 7: 周总结
├── 整理代码阅读笔记
├── 更新系统理解图
└── 确认：能解释"训练是怎么跑的"

Milestone: 能从头到尾解释 SFT 流程

================== Phase 3: 深入理解 + 实验（Week 3）==================

目标：通过改代码来验证理解

Day 1-2: 超参实验
├── 改 lr、batch size
├── 记录对 loss 和成功率的影响
└── 产出：超参敏感性分析

Day 3-4: 架构实验
├── 冻结 VLM，只训 action head
├── 对比全量微调
├── 理解 VLM 的作用
└── 产出：VLM 作用分析

Day 5-6: 深入 flow matching
├── 读 flow matching 相关代码
├── 理解 noise schedule、loss 计算
├── 尝试改一些 flow matching 的参数
└── 产出：flow matching 原理笔记

Day 7: 周总结 + 中期调整
├── 评估进度
├── 调整后续计划（如果需要）
└── 确认：能解释 flow matching 的核心思想

Milestone: 能修改模型并预测结果

================== Phase 4: RL 基础（Week 4）==================

目标：为 RFT 打基础

Day 1-2: RL 概念学习
├── 看李宏毅/HuggingFace 的 Policy Gradient 讲解
├── 理解 reward → advantage → policy update
└── 产出：RL 基础概念笔记

Day 3-4: PPO 实践
├── 用 CleanRL 在 CartPole 上跑 PPO
├── 读 PPO 代码（单文件，~300行）
├── 理解 clipped surrogate objective
└── 产出：PPO 代码注释版

Day 5-6: Diffusion/Flow RL 了解
├── 读 DPPO 论文（重点看 method）
├── 了解为什么 flow model 需要特殊处理
├── 浏览 ReinFlow/FPO 的核心思想
└── 产出：Flow RL 方法对比笔记

Day 7: 周总结
├── 整理 RL 学习笔记
├── 画出 RL fine-tuning 的流程图
└── 确认：能解释为什么 VLA 不能直接用 PPO

Milestone: 理解 RL 基础，知道 Flow RL 的难点

================== Phase 5: RFT 初探（Week 5）==================

目标：尝试最简单的 RL 改进

Day 1-2: 设计 reward
├── 在 LIBERO 中设计简单的 reward（比如 distance-based）
├── 写 reward 计算代码
└── 产出：reward function 实现

Day 3-4: Reward-weighted SFT
├── 收集一些轨迹，标记 success/failure
├── 实现 weighted loss（成功样本权重高）
├── 这是最简单的"伪 RL"
└── 产出：reward-weighted training 代码

Day 5-6: 实验和分析
├── 对比普通 SFT 和 reward-weighted SFT
├── 分析是否有改进
├── 思考下一步怎么做
└── 产出：RFT 初步实验报告

Day 7: 周总结
├── 整理 RFT 相关笔记
├── 评估这个方向的可行性
└── 规划后续研究方向

Milestone: 完成一个最简单的 RL 改进实验

================== Phase 6: 收尾 + 延伸（Week 6，如果有）==================

Option A: 继续深入 RFT
├── 尝试更复杂的 RL 方法
├── 读更多 Flow RL 论文
└── 可能的话实现 DPPO 的简化版

Option B: 论文阅读 + 迁移思考
├── 读许华哲推荐的论文
├── 思考哪些 idea 可以迁移到 SmolVLA
└── 写一份 research proposal

Option C: 补充基础
├── 如果前面有卡住的地方
├── 花时间补齐
└── 确保基础扎实
```

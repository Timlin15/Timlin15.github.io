首先是研究了[[WholebodyVLA]]和[[HumanPlus]]人形机器人领域的内容，调研了PI05移植到宇树G1上的可行性，作为长期目标，有进展可以填充进[[pi0.5移植到宇树g1可行性研究]]。

这周首先在Mujoco上部署了G1的仿真，用一个adapter转化了PI05的输出到G1上：
![[unitreeg1_pi05.mp4]]
可以看到效果非常差，因为PI05是为了18DoF左右的训练的，也没有灵巧手的数据，动作的维度对不齐。

总的来说，我觉得移植可以有以下几个方向可以考虑：
1. 用这些数据完全重新训练一个动作专家，保持VLM不动，使得维度可以生成全身动作 
2. 固定PI05，对其进行lora微调，下半身用RL协调，因为lerobot中最大输出维度是32维，而G1 DoF是23-46，有可能超出最大输出维度。
3. 像WholeBody VLA那样，让VLM输出全身的latent action，上半身过flow model，下半身过RL 

3的难度不小，特别是涉及修改tokenizer这些VLM的东西。
因此打算先从Lora微调开始，如果是使用[dex1](https://www.unitree.com/cn/Dex1-1)，也就是夹具，自由度会是
，再探索1和3这两个方向。
另外他两都支持Lerobot，因此这周也有相当的时间在看Lerobot的模型建模和推理/训练的pipeline。见[[PI05建模]]和[[脚本分析]]


### 数据集
灵巧手数据：

| **数据集名称**     | **总轨迹时长/规模**   | **类型**            | **关键特征**                           |
| ------------- | -------------- | ----------------- | ---------------------------------- |
| **EgoDex**    | **829 小时**     | 真实（Egocentric）    | Apple Vision Pro采集，原生3D手部追踪，194种任务 |
| **DexCanvas** | **7,000 小时**   | 混合（Real+Sim）      | 70小时真实种子数据扩展，含物理接触力标注              |
| **Dex1B**     | **10亿 条轨迹**    | 合成                | 生成式模型生成，极端多样性，用于大规模预训练             |
| **HOI4D**     | **~44.4 小时**   | 真实（4D Egocentric） | 2.4M帧 @ 15fps，类级别物体交互              |
| **ARCTIC**    | **~11.4 小时**   | 真实（MoCap）         | 双手协同，关节物体操作，SMPL-X/MANO标注          |
| **DexYCB**    | **5.0 小时**     | 真实（RGB-D+MoCap）   | YCB物体抓取与交接，Sim-to-Real基准           |
| **OakInk**    | **~2.9 小时**    | 真实（图像/MoCap）      | 31.4万帧，强调物体启示性（Affordance）         |
| **RealDex**   | **~4 小时** (推算) | 真实（遥操作）           | 真实机器人遥操作数据，捕捉动态抓取行为                |
G1数据：

| **数据集/项目**            | **总时长/规模**     | **来源**             | **类型** | **关键特征**            | **数据获取**                       |
| --------------------- | -------------- | ------------------ | ------ | ------------------- | ------------------------------ |
| **UnifoLM Corpus**    | **~340 小时**    | 宇树官方               | 真实     | 用于VLA模型训练的集合，含多个子任务 | Hugging Face (unitreerobotics) |
| **Humanoid Everyday** | **~27.7 小时**   | 学术界 (USC/Toyota)   | 真实     | 10.3k轨迹，专注移动操作与人机交互 | Github / Project Page          |
| **G1_Fold_Towel**     | **~2.9 小时**    | 宇树官方               | 真实     | 柔性物体操作专项数据          | Hugging Face                   |
| **G1_Clean_Table**    | **~2.5 小时**    | 宇树官方               | 真实     | 大范围桌面清洁任务           | Hugging Face                   |
| **GR00T Teleop G1**   | **~10 小时** (估) | NVIDIA             | 真实     | 1k轨迹，水果分拣，GR00T微调专用 | Hugging Face (NVIDIA)          |
| **OmniRetarget**      | **~9 小时**      | 学术界 (Stanford/CMU) | 仿真/合成  | 动作重定向，跑酷与复杂运动       | Project Page                   |

---
被批了，和组内的VLA方向不一样，说应该统一方向，这样感觉找不到研究方向，去骚扰师兄了
```
师兄，我最近在思考自己的研究方向，有些困惑想请教一下。
目前的问题是感觉缺少一个一个长期目标/项目，导致
    1.学习比较碎片化，这边学一点那边学一点，深度不够
    2.在组会上也找不到一个有连续性的主题来讲。我看组会上都是围绕自己的论文或者毕设/大创来讲的
同时师兄这边的项目我能力不够，跟不上项目迭代速度的要求，我们同组的大创VLA模型这块现在主要还是复现，撑不起两人汇报的内容。

我的想法：
我对模型架构比较感兴趣，想延续寒假开始的那次汇报，系统研究lerobot的infra pipeline。但我担心这个方向在组会上不太好汇报，主要是理解代码，修改较少。

所以我的疑问是：这个方向可行吗？本科生在组会的汇报对论文/实验产出要求大吗。如果不太合适，你觉得我现在应该优先做什么？
```
师兄叫我先去看看RoboTwin的train/dataloader这些文件。最后下来和[[大二寒假展望]]也差不多。
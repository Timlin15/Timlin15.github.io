> [!NOTE] Reference
> 参考了[Step-by-Step Diffusion: An Elementary Tutorial | PDF](https://arxiv.org/pdf/2406.08929)，[Flow Matching Guide and Code | PDF](https://arxiv.org/pdf/2412.06264)，[An Introduction to Flow Matching and Diffusion Models | PDF](https://arxiv.org/pdf/2506.02070)

## Diffusion model
生成式模型的**目标**是：
> 从多个在未知分布$p^*(x)$中独立同分布的数据中，提取出一个新的样本，是从分布$p^*$中提取的

这一种简单的思路，在高阶上，是生成一种转化，把易于采样的分布(如正态分布)转为目标分布$p^*$。Diffusion为生成这种**转化**提供了一种通用模板，其精髓在于把从未知分布$p^*$中采样归纳为一系列简单采样。详见以下的Gaussian Diffusion

### Gaussian Diffusion
对于高斯扩散过程，令 $x_0$ 为 $\mathbb{R}^d$ 中服从目标分布 $p^*$（例如狗的图像）的随机变量。然后通过依次添加具有较小尺度 $\sigma$ 的独立高斯噪声，构造一列随机变量 $x_1, x_2, \ldots, x_T$： $$ \begin{aligned} x_{t+1} &:= x_t + \eta_t, \\ \eta_t &\sim \mathcal{N}(0, \sigma^2). \end{aligned} \tag{1} $$ 这被称为**前向过程**，它将数据分布转化为噪声分布。公式 (1) 定义了所有 $(x_0, x_1, \ldots, x_T)$ 上的联合分布，我们记 $\{p_t\}_{t \in [T]}$ 为每个 $x_t$ 的边缘分布。注意到当步数 $T$ 较大时，分布 $p_T$ 近似于高斯分布，因此我们可以仅通过采样一个高斯分布来近似地从 $p_T$ 中采样。
实际上，由于不断地加上正态分布会使得方差无限放大，所以会通过加权不同分布的权重来缩放：
$$
x_t = \sqrt{1 - \beta_t} x_{t-1} + \sqrt{\beta_t} \epsilon_t
$$


![image.png](https://typora-1344509263.cos.ap-guangzhou.myqcloud.com/test/20251025160740.png)
现在，假设我们可以解决以下子问题：

> “给定一个服从边缘分布 $p_t$ 的样本，生成一个服从边缘分布 $p_{t-1}$ 的样本。”

我们将能够实现这一目标的方法称为**反向采样器**，因为它告诉我们如何在已知可以从 $p_t$ 中采样的前提下，从 $p_{t-1}$ 中采样。如果我们拥有一个反向采样器，就可以从 $p_T$ 中采样一个高斯样本作为起点，然后迭代地应用反向采样过程，依次得到来自 $p_{T-1}, p_{T-2}, \ldots$ 的样本，最终得到 $p_0 = p^*$。

扩散模型的关键思想是：**学习逆转每一个中间步骤可能比一次性从目标分布中采样更容易**。构造反向采样器的方法有很多，但为了具体起见，我们首先介绍标准的扩散采样器，称之为 **DDPM 采样器**。

理想的 DDPM 采样器采用一种直观的策略：在时间步 $t$，给定输入 $z$（保证是从 $p_t$ 中采样的样本），我们输出一个来自条件分布

$$
p(x_{t-1} \mid x_t = z)
\tag{2}
$$

的样本。这显然是一个正确的反向采样器。问题是，它需要为每个 $x_t$ 学习一个生成模型来建模条件分布 $p(x_{t-1} \mid x_t)$，这可能很复杂。但如果每一步的噪声 $\sigma$ 足够小，那么这个条件分布会变得简单：

**事实 1（扩散反向过程）**。对于较小的 $\sigma$，以及在 (1) 中定义的高斯扩散过程，条件分布 $p(x_{t-1} \mid x_t)$ 本身接近于高斯分布。也就是说，对所有时间 $t$ 和条件值 $z \in \mathbb{R}^d$，存在某个均值参数 $\mu \in \mathbb{R}^d$，使得

$$
p(x_{t-1} \mid x_t = z) \approx \mathcal{N}(x_{t-1};\ \mu,\ \sigma^2).
\tag{3}
$$

![image.png](https://typora-1344509263.cos.ap-guangzhou.myqcloud.com/test/20251025161523.png)
这不是一个显而易见的事实；我们将在第 2.1 节中推导它。这一事实带来了极大的简化：我们不再需要从零开始学习任意分布 $p(x_{t-1} \mid x_t)$，现在我们对这个分布的了解仅缺少其均值，我们将其记为 $\mu_{t-1}(x_t)$。当 $\sigma$ 足够小时，我们可以将后验分布近似为高斯分布，这一点在图 2 中有所说明。这是一个重要观点，因此再次强调：对于给定的时间 $t$ 和条件值 $x_t$，学习条件分布 $p(x_{t-1} \mid x_t)$ 的均值就足以学习整个条件分布。

学习 $p(x_{t-1} \mid x_t)$ 的均值比学习完整的条件分布要简单得多，因为我们可以通过回归来解决这个问题。具体来说，我们有一个联合分布 $(x_{t-1}, x_t)$，从中可以轻松采样，我们希望估计 $\mathbb{E}[x_{t-1} \mid x_t]$。这可以通过优化一个标准的回归损失来实现：

$$
\begin{aligned}
\mu_{t-1}(z) &:= \mathbb{E}[x_{t-1} \mid x_t = z] \\
&\Longrightarrow \mu_{t-1} = argmin_{f: \mathbb{R}^d \to \mathbb{R}^d} \mathbb{E}_{x_t, x_{t-1}} \left\| f(x_t) - x_{t-1} \right\|_2^2 \\
&= argmin_{f: \mathbb{R}^d \to \mathbb{R}^d} \mathbb{E}_{x_{t-1}, \eta} \left\| f(x_{t-1} + \eta_t) - x_{t-1} \right\|_2^2,
\end{aligned}
\tag{4-6}
$$

其中期望是针对从目标分布 $p^*$ 中采样的样本 $x_0$ 计算的。这种特定的回归问题在某些设定下已被广泛研究。例如，当目标分布 $p^*$ 是图像上的分布时，对应的回归问题（公式 (6)）恰好是一个**图像去噪目标**，可以用熟悉的手段（如卷积神经网络）来处理。

**回溯来看**，我们已经看到了一个惊人的结论：我们将“从任意分布中学习采样”这一复杂问题，简化为了标准的回归问题。
### 1.2 抽象中的扩散模型

现在让我们抽象掉高斯设定，以一种能够捕捉多种变体（包括确定性采样器、离散域和流匹配）的方式定义扩散类模型。

抽象地讲，构建一个扩散类生成模型的方法如下：我们从目标分布 $p^*$ 出发，并选择一个易于采样的基分布 $q(x)$，例如标准高斯分布或独立同分布的比特。然后我们尝试构造一列分布，这些分布在目标分布 $p^*$ 和基分布 $q$ 之间进行插值。也就是说，我们构造一系列分布：

$$
p_0,\ p_1,\ p_2,\ \ldots,\ p_T,
\tag{7}
$$


> [!NOTE] 来源
> 本文全部由Gemini3 Pro生成

## **1. 执行摘要：机器人学习的数据新范式**

在通往通用人工智能（AGI）的征途中，具身智能（Embodied AI）正处于爆发的前夜。与大语言模型（LLM）受益于互联网级文本数据、计算机视觉（CV）受益于海量图像数据不同，机器人操作——尤其是涉及多指灵巧手（Dexterous Hand）的精细操作和人形机器人的全身控制（Whole-Body Control）——长期面临着严峻的“数据荒”（Data Scarcity）。本报告旨在为该领域的专业研究人员提供一份详尽的指南，深入剖析当前开源社区中针对灵巧手操作及宇树（Unitree）G1人形机器人的核心数据集资源，特别是关于其轨迹时长（Trajectory Duration）、数据模态及技术规格的详细统计。

通过对数百份技术文档、论文及代码仓库的深度挖掘与交叉验证，本报告揭示了数据收集方法论的重大范式转移：从早期昂贵且难以扩展的传统动作捕捉（MoCap），向基于消费级XR设备（如Apple Vision Pro）的大规模第一人称视频采集，以及基于强化学习的“Sim-to-Real”混合数据生成演进。针对宇树G1这一新兴的高性能人形机器人平台，数据生态正在以惊人的速度成熟，从早期的动作重定向（Retargeting）发展为包含数百万帧、覆盖移动操作（Loco-manipulation）的原生遥操作数据集。

本报告主要分为两个核心部分：首先是灵巧手操作数据集的详尽图谱，涵盖从传统的数小时MoCap数据到全新的数千小时混合数据；其次是宇树G1人形机器人的全方位数据解析，重点分析其在全身控制与操作任务中的数据现状与轨迹规模。

## **2. 灵巧手操作开源数据集全景解析**

灵巧手操作被视为机器人操作领域的“圣杯”。与简单的二指夹爪不同，多指灵巧手拥有极高的自由度（DoF），这使得其状态空间呈指数级增长，对训练数据的质量和数量提出了苛刻要求。以下分析根据数据生成机制与规模，将现有的开源数据集分为三个层级。

### **2.1 第一层级：大规模第一人称视频与混合现实数据集**

这一层级代表了当前数据规模的巅峰，主要利用被动视频数据或先进的VR/MR设备进行低成本、大规模采集。

#### **2.1.1 EgoDex：灵巧操作数据的“ImageNet”时刻**

**总轨迹时长：829小时** 1

- **数据规模与构成：** EgoDex 是目前已知规模最大、多样性最强的灵巧手操作数据集。它包含 **829小时** 的第一人称（Egocentric）视频，总计约 **9000万帧**（30Hz采样），涵盖 **33.8万个片段（Episodes）** 4。
- **技术突破：** 传统数据集（如Ego4D）虽然规模巨大，但缺乏精确的手部姿态标注。EgoDex 利用 **Apple Vision Pro** 的先进传感器套件，在录制视频的同时，直接获取了高精度的 **3D手部及手指骨骼跟踪数据**、头部姿态以及环境的SLAM数据 1。
- **任务多样性：** 数据集覆盖了 **194种** 不同的桌面操作任务，不仅包括简单的抓取，还囊括了系鞋带、折叠衣物等极具挑战性的双手协同任务，涉及日常生活中的各种刚体与柔性物体 2。
- **深度洞察：** EgoDex 的出现标志着数据采集从“实验室动作捕捉”向“穿戴式现实捕捉”的跨越。829小时的数据量级远超传统数据集，为训练能够泛化到不同光照、背景和手部形态的“操作基础模型”（Foundation Models for Manipulation）提供了可能。

#### **2.1.2 DexCanvas：物理仿真驱动的数据倍增**

**总轨迹时长：7,000小时（混合）** 6

- **数据构成：** DexCanvas 采用了一种独特的“混合实虚”（Hybrid Real-Synthetic）构建策略。其核心种子数据是 **70小时** 的高质量真人动作捕捉数据（约12,000条序列） 7。
- **扩展机制：** 研究人员开发了一套基于强化学习（RL）的“Real-to-Sim”管道。该管道不仅将真人动作重定向到虚拟灵巧手，还通过物理仿真生成了大量变体。这一过程将原始数据扩充了100倍，最终形成了总计 **7,000小时** 的物理一致性操作数据 6。
- **核心价值：** 纯视觉数据集往往缺乏接触力（Contact Force）信息。DexCanvas 的仿真部分提供了每一帧的精确 **接触力标注** 和物理状态，这对于学习鲁棒的动力学控制策略至关重要。它基于Cutkosky分类法定义了21种基本操作类型，确保了技能的系统覆盖 6。

#### **2.1.3 Dex1B：十亿级合成数据**

**总轨迹时长/规模：10亿条演示（1 Billion Demonstrations）** 11

- **性质：** 这是一个完全合成的数据集，旨在探索数据规模的极限。
- **生成方法：** 利用名为 **DexSimple** 的生成式模型，基于少量优化生成的种子数据进行训练，从而能够自动生成海量的抓取和关节操作（Articulation）轨迹 11。
- **规模意义：** 虽然不以“小时”计量，但10亿条轨迹的数量级完全打破了现有记录。这表明在物体几何和抓取姿态的多样性穷举上，合成数据具有不可替代的优势。该数据集主要用于预训练视觉-动作策略，以解决Sim-to-Real中的泛化问题 14。

### **2.2 第二层级：高精度动作捕捉与多模态数据集**

这一层级的数据集通常在受控的实验室环境中采集，拥有最精确的Ground Truth（如物体6D位姿、亚毫米级手部骨骼），是评估算法精度的基准。

#### **2.2.1 HOI4D：4D类级别交互基准**

**总轨迹时长：约 44.4 小时** 15

- **计算依据：** 数据集包含 **240万帧**（2.4M RGB-D frames），采集频率为 **15 fps**。

- 计算公式：$2,400,000 \text{ frames} \div 15 \text{ fps} \div 3600 \text{ sec/hour} \approx 44.44 \text{ hours}$。

- **内容特征：** 包含4000个序列，由9名参与者与800个不同的物体实例（涵盖16个类别）进行交互 16。
- **技术细节：** 数据集提供了全景分割、运动分割、3D手部姿态以及类级别的物体6D位姿标注。它特别关注刚体和关节物体（如笔记本电脑、抽屉）的交互，弥补了以往数据集仅关注静态抓取的不足。

#### **2.2.2 ARCTIC：双手关节物体灵巧操作**

**总轨迹时长：约 11.4 小时** 17

- **数据规模：** 包含 **210万帧**（2.1M frames）视频 18。虽然有些文献在宽泛背景下提及“51小时” 20，但核心交互序列的有效时长通常被引用为 **11.4小时** 17。
- **核心焦点：** 专注于 **双手（Bimanual）** 对 **关节物体（Articulated Objects）** 的灵巧操作。任务包括打开盒子、操作剪刀、使用搅拌机等，这些任务要求双手的高度协同以及手与物体之间复杂的动态接触 18。
- **标注质量：** 提供了高精度的SMPL-X全身模型、MANO手部模型以及物体网格的序列化标注，支持接触重建和交互场估计等前沿任务。

#### **2.2.3 DexYCB：实物抓取与交接基准**

**总轨迹时长：5.0 小时** 21

- **规模：** 包含1,000个试次（Trials），涉及10名受试者和20个YCB标准物体。
- **内容：** 每个试次约3-4秒，涵盖了从桌面抓取物体到保持、再到人机交接（Handover）的全过程 23。
- **地位：** DexYCB 是“Sim-to-Real”研究中的标准“Real”验证集。许多在合成数据（如DexGraspNet或Dex1B）上训练的策略，最终都会在DexYCB上进行微调或评估，以测试其现实世界的适用性 13。

#### **2.2.4 OakInk：启示性（Affordance）与意图理解**

**总轨迹时长：约 2.9 小时**

- **计算依据：** **OakInk-Image** 子集包含 **314,404帧** 25。假设采用标准的30fps录制，时长约为2.9小时。
- **结构：** 由 *OakBase*（物体启示性知识库）和 *InkBase*（人类交互知识库）组成。包含50,000个独特的“手-物”交互姿态对 26。
- **独特价值：** 该数据集强调“意图”（Intent）。同一个杯子，是为了“喝水”抓取还是为了“递给别人”抓取，手势是不同的。OakInk捕捉了这种基于功能性的细微差别，对于让机器人理解操作意图至关重要 27。

### **2.3 灵巧手数据集对比汇总表**

| **数据集名称** | **总轨迹时长/规模** | **类型**              | **关键特征**                                    | **数据来源引用** |
| -------------- | ------------------- | --------------------- | ----------------------------------------------- | ---------------- |
| **EgoDex**     | **829 小时**        | 真实（Egocentric）    | Apple Vision Pro采集，原生3D手部追踪，194种任务 | 1                |
| **DexCanvas**  | **7,000 小时**      | 混合（Real+Sim）      | 70小时真实种子数据扩展，含物理接触力标注        | 6                |
| **Dex1B**      | **10亿 条轨迹**     | 合成                  | 生成式模型生成，极端多样性，用于大规模预训练    | 11               |
| **HOI4D**      | **~44.4 小时**      | 真实（4D Egocentric） | 2.4M帧 @ 15fps，类级别物体交互                  | 15               |
| **ARCTIC**     | **~11.4 小时**      | 真实（MoCap）         | 双手协同，关节物体操作，SMPL-X/MANO标注         | 17               |
| **DexYCB**     | **5.0 小时**        | 真实（RGB-D+MoCap）   | YCB物体抓取与交接，Sim-to-Real基准              | 21               |
| **OakInk**     | **~2.9 小时**       | 真实（图像/MoCap）    | 31.4万帧，强调物体启示性（Affordance）          | 25               |
| **RealDex**    | **~4 小时** (推算)  | 真实（遥操作）        | 真实机器人遥操作数据，捕捉动态抓取行为          | 29               |

*(注：RealDex的时长基于文献中提及的牙齿模型操作案例时长及典型试次推算，其核心贡献在于捕捉真实的遥操作行为而非单纯时长)*

## **3. 宇树G1人形机器人全身数据与开源生态深度解析**

宇树G1作为一款高性价比（约1.6万美元）、具备高动态运动能力的人形机器人，正在迅速成为学术界和工业界的首选科研平台。与仅关注手部的研究不同，G1的数据需求集中在 **全身控制（Whole-Body Control, WBC）** 和 **移动操作（Loco-manipulation）** 上，即如何在保持双足平衡和移动的同时，利用上肢和灵巧手完成复杂任务。

### **3.1 核心数据集：Humanoid Everyday**

这是目前针对宇树G1（及H1）最全面、最具代表性的开源数据集。

- **总轨迹时长：约 27.7 小时**

- **数据来源分析：** 数据集包含 **10,300条轨迹**（10.3k trajectories），总计 **超过300万帧**（3 million frames），采集频率为 **30Hz** 31。
- **计算：** $3,000,000 \text{ frames} \div 30 \text{ fps} = 100,000 \text{ seconds} \approx 27.77 \text{ hours}$。

- **数据内容：** 专注于 **“开放世界人形机器人操作”**。它不仅仅是站立抓取，而是包含了大量 **Loco-manipulation**（移动操作）任务，要求机器人协调下肢行走与上肢操作。
- **任务分类：** 覆盖7大类、260种具体任务，包括：

- 移动操作（Loco-Manipulation）
- 柔性物体操作（Deformable Manipulation，如叠毛巾）
- 关节物体操作（Articulated Manipulation，如开门、开柜子）
- 工具使用（Tool Use）
- 人机交互（Human-Robot Interaction）等 32。

- **采集方式：** 利用Apple Vision Pro等设备构建的高效遥操作流水线，实现了全身姿态的同步映射与采集，包含RGB、深度、LiDAR点云及触觉等多模态数据 31。

### **3.2 宇树官方开源数据集（Hugging Face - unitreerobotics）**

宇树机器人官方在Hugging Face上建立了一个名为 unitreerobotics 的组织，开源了大量基于G1机器人的真实操作数据。这些数据主要用于支持其 **UnifoLM-VLA-0** 视觉-语言-动作模型的训练。

- **总体规模（UnifoLM语料库）：** 宇树官方披露，其VLA模型的训练集包含 **约340小时** 的高质量真实机器人数据 34。这其中大部分数据来源于G1机器人的实际操作录制。
- **具体数据集详情与时长估算：** 官方仓库中列出了多个针对特定任务的子数据集。以下是基于样本数（Samples）和标准采样率（30Hz，参考G1_Pack_PingPong的元数据 37）对各子数据集时长的详细估算：

| **数据集名称 (Repository Name)** | **样本数 (Samples/Frames)** | **估算时长 (小时) @ 30fps** | **任务描述**                              |
| -------------------------------- | --------------------------- | --------------------------- | ----------------------------------------- |
| G1_Fold_Towel                    | 311,000                     | **~2.88 小时**              | 柔性物体操作：折叠毛巾 38                 |
| G1_Clean_Table                   | 266,000                     | **~2.46 小时**              | 桌面清洁任务，涉及擦拭动作 38             |
| G1_Dex1_MountCamera              | 390,000                     | **~3.61 小时**              | 头戴/挂载相机视角的灵巧操作 40            |
| G1_Organize_Tools                | 183,000                     | **~1.69 小时**              | 工具整理与摆放 38                         |
| G1_DualRobot_Clean               | 171,000                     | **~1.58 小时**              | 双臂/双机器人协同清洁 38                  |
| G1_Pack_PencilBox                | 163,000                     | **~1.51 小时**              | 精细操作：将文具放入铅笔盒 38             |
| G1_Pack_PingPong                 | 161,000                     | **~1.49 小时**              | 动态物体：收纳乒乓球拍 25                 |
| G1_Erase_Board                   | 128,000                     | **~1.19 小时**              | 擦黑板，涉及力控与大范围手臂运动 38       |
| G1_Prepare_Fruit                 | 124,000                     | **~1.15 小时**              | 水果准备（抓取、放置） 38                 |
| G1_Wipe_Table                    | 75,900                      | **~0.70 小时**              | 单一动作擦桌子 27                         |
| **总计 (仅上述列出部分)**        | **~2,000,000+**             | **~18+ 小时**               | *注：UnifoLM总库包含更多未单独列出的数据* |

- **数据格式：** 这些数据采用了 **LeRobot** 格式标准（Hugging Face推出的开源机器人学习框架），极大降低了开发者的使用门槛 42。数据通常包含同步的RGB视频、关节状态（Joint States）和动作指令。

### **3.3 NVIDIA GR00T 生态数据**

NVIDIA 为其 GR00T 人形机器人基础模型发布了针对 G1 的特定微调数据集。

- **数据集名称：** PhysicalAI-Robotics-GR00T-Teleop-G1 44。
- **规模：** **1,000条** 遥操作轨迹。
- **时长估算：** 假设每条轨迹平均30-40秒（典型抓取任务时长 37），该数据集总时长约为 **8-11小时**。
- **内容：** 专注于使用G1的上半身和三指灵巧手进行水果（苹果、梨等）的拾取与放置任务。
- **技术规格：** 20fps，640x480分辨率，包含43维的状态向量（全身关节+手部）45。

### **3.4 仿真与重定向数据（Sim-to-Real）**

除了真实数据，学术界还通过将人类动作捕捉数据（如AMASS）重定向到G1机器人上，生成了大量仿真数据。

- **OmniRetarget / TrajBooster：**

- **时长：** 生成了超过 **9小时** 的高质量、符合运动学约束的G1移动操作轨迹 46。
- **价值：** 这些数据解决了从人类形态到机器人形态的“实施差距”（Embodiment Gap），使得机器人能够通过模仿人类视频学习跑酷、攀爬和复杂操作，仅需极少量（约10分钟）的真实数据进行微调即可部署 48。

### **3.5 宇树G1数据资源汇总表**

| **数据集/项目**            | **总时长/规模**     | **来源**             | **类型** | **关键特征**            | **数据获取**                       |
| --------------------- | -------------- | ------------------ | ------ | ------------------- | ------------------------------ |
| **UnifoLM Corpus**    | **~340 小时**    | 宇树官方               | 真实     | 用于VLA模型训练的集合，含多个子任务 | Hugging Face (unitreerobotics) |
| **Humanoid Everyday** | **~27.7 小时**   | 学术界 (USC/Toyota)   | 真实     | 10.3k轨迹，专注移动操作与人机交互 | Github / Project Page          |
| **G1_Fold_Towel**     | **~2.9 小时**    | 宇树官方               | 真实     | 柔性物体操作专项数据          | Hugging Face                   |
| **G1_Clean_Table**    | **~2.5 小时**    | 宇树官方               | 真实     | 大范围桌面清洁任务           | Hugging Face                   |
| **GR00T Teleop G1**   | **~10 小时** (估) | NVIDIA             | 真实     | 1k轨迹，水果分拣，GR00T微调专用 | Hugging Face (NVIDIA)          |
| **OmniRetarget**      | **~9 小时**      | 学术界 (Stanford/CMU) | 仿真/合成  | 动作重定向，跑酷与复杂运动       | Project Page                   |

## **4. 深度洞察与趋势分析**

### **4.1 数据采集的“苦涩教训”与范式重构**

机器人领域正在经历人工智能的“苦涩教训”（The Bitter Lesson）：**利用大规模算力和大规模数据的通用方法，最终会胜过利用人类先验知识的精巧设计**。

- **现象：** 从DexYCB的5小时到EgoDex的829小时，再到DexCanvas的7000小时，数据规模呈现指数级增长。这表明单纯依赖实验室内的精密动作捕捉（MoCap）已无法满足需求。
- **趋势：** 未来的主流将是 **“低成本、大规模、含噪声”** 的数据。Apple Vision Pro等消费级设备的普及，使得非专业人员也能以极低成本贡献高质量的3D手部追踪数据，这直接促成了EgoDex和Humanoid Everyday等数据集的诞生。

### **4.2 全身协同（Loco-manipulation）成为新高地**

宇树G1的数据生态揭示了人形机器人研究重心的转移。

- **过去：** 研究往往将“移动”（底盘）和“操作”（机械臂）解耦，分别训练。
- **现在：** Humanoid Everyday 和 OmniRetarget 等数据集强调 **全身协同**。机器人需要在行走过程中保持平衡并完成操作（如边走边端水）。这种任务无法简单拆解，必须依赖包含全身状态（从脚踝到手指）的端到端数据。G1作为低成本硬件的普及，加速了这一领域数据的生成和算法的迭代。

### **4.3 官方下场与标准化生态**

宇树机器人不仅仅提供硬件，更通过开源 UnifoLM 数据集和适配 LeRobot 框架，试图建立数据标准。

- **标准化：** 过去机器人数据格式混乱（HDF5, Pickle, ROSBag各行其道）。宇树拥抱 Hugging Face 的 LeRobot 格式，意味着其数据可以直接被社区现有的Transformer策略（如ACT, Diffusion Policy）加载训练。这种“硬件+数据+标准框架”的垂直整合，极大地降低了具身智能的研究门槛，可能会像ROS当年统一操作系统一样，统一机器人学习的数据接口。

### **4.4 仿真数据作为“倍增器”**

尽管真实数据（Real Data）无可替代，但 **DexCanvas (7000h)** 和 **Dex1B (10亿条)** 证明了仿真数据的核心地位。

- **策略：** 利用少量真实数据作为“种子”（Seed），通过物理仿真和强化学习进行大规模变体生成（Domain Randomization），是目前解决数据匮乏最经济高效的路径。对于宇树G1这样复杂的系统，先在仿真中利用重定向数据（如OmniRetarget）预训练，再用少量真实数据（如UnifoLM）微调，已成为标准的“Sim-to-Real”范式。

## **5. 结论**

当前，灵巧手操作与人形机器人领域正处于数据爆炸的初期。**EgoDex (829h)** 和 **DexCanvas (7000h)** 为灵巧手操作确立了新的规模标杆，证明了视觉被动采集和仿真合成是突破数据瓶颈的关键。而在人形机器人领域，**宇树G1** 凭借其高性价比和开放性，迅速聚集了包括 **Humanoid Everyday (~27.7h)** 和 **UnifoLM (~340h)** 在内的丰富数据资源。这些数据不仅数量可观，更在质量上实现了从单一模态向多模态、从静态操作向全身移动操作的跨越。对于研究人员而言，善用这些开源资源，结合Sim-to-Real技术，将是未来3-5年内实现具身智能突破的必由之路。

#### **引用的著作**

1. EgoDex: Learning Dexterous Manipulation from Large-Scale Egocentric Video, 访问时间为 二月 3, 2026， https://machinelearning.apple.com/research/egodex-learning-dexterous-manipulation
2. EgoDex: Learning Dexterous Manipulation from Large-Scale Egocentric Video - arXiv, 访问时间为 二月 3, 2026， https://arxiv.org/html/2505.11709v1
3. EgoDex: Learning Dexterous Manipulation from Large-Scale Egocentric Video - GitHub, 访问时间为 二月 3, 2026， https://github.com/apple/ml-egodex
4. EgoDex: Learning Dexterous Manipulation from Large-Scale Egocentric Video - arXiv, 访问时间为 二月 3, 2026， [https://arxiv.org/pdf/2505.11709?](https://arxiv.org/pdf/2505.11709)
5. Peide Huang - CatalyzeX, 访问时间为 二月 3, 2026， [https://www.catalyzex.com/author/Peide%20Huang](https://www.catalyzex.com/author/Peide Huang)
6. DexCanvas: Bridging Human Demonstrations and Robot Learning for Dexterous Manipulation - arXiv, 访问时间为 二月 3, 2026， https://arxiv.org/html/2510.15786v1
7. DexCanvas: Bridging Human Demonstrations and Robot Learning for Dexterous Manipulation | alphaXiv, 访问时间为 二月 3, 2026， https://www.alphaxiv.org/overview/2510.15786v1
8. [2510.15786] DexCanvas: Bridging Human Demonstrations and Robot Learning for Dexterous Manipulation - arXiv, 访问时间为 二月 3, 2026， https://arxiv.org/abs/2510.15786
9. DEXROBOT/DexCanvas · Datasets at Hugging Face, 访问时间为 二月 3, 2026， https://huggingface.co/datasets/DEXROBOT/DexCanvas
10. DexCanvas: Bridging Human Demonstrations and Robot Learning for Dexterous Manipulation - arXiv, 访问时间为 二月 3, 2026， https://arxiv.org/pdf/2510.15786
11. Dex1B: Learning with 1B Demonstrations for Dexterous Manipulation - OpenReview, 访问时间为 二月 3, 2026， https://openreview.net/pdf/014b3145c9c44e09b6252c59864d3ea2e163317a.pdf
12. Dex1B - Jianglong Ye, 访问时间为 二月 3, 2026， https://jianglongye.com/dex1b/
13. Dex1B: Learning with 1B Demonstrations for Dexterous Manipulation - arXiv, 访问时间为 二月 3, 2026， https://arxiv.org/html/2506.17198v1
14. aapatni/robotics-deep-dive-2025 - GitHub, 访问时间为 二月 3, 2026， https://github.com/aapatni/robotics-deep-dive-2025
15. HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction - arXiv, 访问时间为 二月 3, 2026， https://arxiv.org/html/2203.01577v4
16. A 4D Egocentric Dataset for Category-Level Human-Object Interaction - HOI4D, 访问时间为 二月 3, 2026， https://hoi4d.github.io/HOI4D_cvpr2022.pdf
17. Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions - arXiv, 访问时间为 二月 3, 2026， https://arxiv.org/html/2508.04681v1
18. ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation - Max-Planck-Gesellschaft, 访问时间为 二月 3, 2026， https://download.is.tue.mpg.de/arctic/arctic_april_24.pdf
19. ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation - CVF Open Access, 访问时间为 二月 3, 2026， https://openaccess.thecvf.com/content/CVPR2023/papers/Fan_ARCTIC_A_Dataset_for_Dexterous_Bimanual_Hand-Object_Manipulation_CVPR_2023_paper.pdf
20. Object-Centric Dexterous Manipulation from Human Motion Data - GitHub, 访问时间为 二月 3, 2026， https://raw.githubusercontent.com/mlresearch/v270/main/assets/chen25f/chen25f.pdf
21. EgoPressure: A Dataset for Hand Pressure and Pose Estimation in Egocentric Vision - arXiv, 访问时间为 二月 3, 2026， https://arxiv.org/html/2409.02224v2
22. EgoPressure: A Dataset for Hand Pressure and Pose Estimation in Egocentric Vision - Paul Streli, 访问时间为 二月 3, 2026， https://paulstreli.com/assets/papers/cvpr2025-egopressure.pdf
23. DexYCB: A Benchmark for Capturing Hand Grasping of Objects, 访问时间为 二月 3, 2026， https://dex-ycb.github.io/assets/chao_cvpr2021.pdf
24. Dex1B: Learning with 1B Demonstrations for Dexterous Manipulation - Jianglong Ye, 访问时间为 二月 3, 2026， https://jianglongye.com/dex1b/static/dex1b.pdf
25. OakInk/docs/datasets.md at main - GitHub, 访问时间为 二月 3, 2026， https://github.com/oakink/OakInk/blob/main/docs/datasets.md
26. [CVPR 2022] OakInk: A Large-scale Knowledge Repository for Understanding Hand-Object Interaction - GitHub, 访问时间为 二月 3, 2026， https://github.com/oakink/OakInk
27. OAKINK2: A Dataset of Bimanual Hands-Object Manipulation in Complex Task Completion - CVF Open Access, 访问时间为 二月 3, 2026， https://openaccess.thecvf.com/content/CVPR2024/papers/Zhan_OAKINK2_A_Dataset_of_Bimanual_Hands-Object_Manipulation_in_Complex_Task_CVPR_2024_paper.pdf
28. OAKINK2 - CVF Open Access, 访问时间为 二月 3, 2026， https://openaccess.thecvf.com/content/CVPR2024/supplemental/Zhan_OAKINK2_A_Dataset_CVPR_2024_supplemental.pdf
29. RealDex: Towards Human-like Grasping for Robotic Dexterous Hand - IJCAI, 访问时间为 二月 3, 2026， https://www.ijcai.org/proceedings/2024/0758.pdf
30. Human Knowledge for Robotic Dexterous ... - HKU Scholars Hub, 访问时间为 二月 3, 2026， https://hub.hku.hk/bitstream/10722/354708/1/FullText.pdf
31. Humanoid Everyday: A Comprehensive Robotic Dataset for Open-World Humanoid Manipulation - arXiv, 访问时间为 二月 3, 2026， https://arxiv.org/html/2510.08807v1
32. Humanoid Everyday: A Comprehensive Robotic Dataset for Open-World Humanoid Manipulation - ResearchGate, 访问时间为 二月 3, 2026， https://www.researchgate.net/publication/396457536_Humanoid_Everyday_A_Comprehensive_Robotic_Dataset_for_Open-World_Humanoid_Manipulation
33. In-N-On: Scaling Egocentric Manipulation with in-the-wild and on-task Data - arXiv, 访问时间为 二月 3, 2026， https://arxiv.org/html/2511.15704v1
34. UnifoLM-VLA-0: Vision-Language-Action Foundation Model, 访问时间为 二月 3, 2026， https://unigen-x.github.io/unifolm-vla.github.io/
35. Yuchu Open Sources UnifoLM-VLA-0 Large Model: Injecting Physical Common Sense into General-Purpose Humanoid Robots - AIBase, 访问时间为 二月 3, 2026， https://www.aibase.com/news/25101
36. Unitree Robotics Open-Sources Multimodal Vision-Language-Action Model：UnifoLM-VLA-0, 访问时间为 二月 3, 2026， https://pandaily.com/unitree-robotics-open-sources-multimodal-vision-language-action-model-unifo-lm-vla-0-1
37. README.md · unitreerobotics/G1_Pack_PingPong at main - Hugging Face, 访问时间为 二月 3, 2026， https://huggingface.co/datasets/unitreerobotics/G1_Pack_PingPong/blob/main/README.md
38. unitreerobotics (Unitree Robotics) - Hugging Face, 访问时间为 二月 3, 2026， https://huggingface.co/unitreerobotics
39. unitreerobotics/G1_Fold_Towel at main - Hugging Face, 访问时间为 二月 3, 2026， https://huggingface.co/datasets/unitreerobotics/G1_Fold_Towel/tree/main
40. unitreerobotics/G1_Dex1_MountCamera_Dataset · Datasets at Hugging Face, 访问时间为 二月 3, 2026， https://huggingface.co/datasets/unitreerobotics/G1_Dex1_MountCamera_Dataset
41. Grasp-and-Lift: Executable 3D Hand-Object Interaction Reconstruction via Physics-in-the-Loop Optimization - arXiv, 访问时间为 二月 3, 2026， https://arxiv.org/html/2601.18121v1
42. LeRobot: Making AI for Robotics more accessible with end-to-end learning - GitHub, 访问时间为 二月 3, 2026， https://github.com/huggingface/lerobot
43. The unitree_il_lerobot open-source project is a modification of the LeRobot open-source training framework, enabling the training and testing of data collected using the dual-arm dexterous hands of Unitree's G1 robot. - GitHub, 访问时间为 二月 3, 2026， https://github.com/unitreerobotics/unitree_IL_lerobot
44. README.md · nvidia/PhysicalAI-Robotics-GR00T-Teleop-G1 at main - Hugging Face, 访问时间为 二月 3, 2026， https://huggingface.co/datasets/nvidia/PhysicalAI-Robotics-GR00T-Teleop-G1/blob/main/README.md
45. nvidia/PhysicalAI-Robotics-GR00T-Teleop-G1 · Datasets at Hugging Face, 访问时间为 二月 3, 2026， https://huggingface.co/datasets/nvidia/PhysicalAI-Robotics-GR00T-Teleop-G1
46. OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction - arXiv, 访问时间为 二月 3, 2026， https://arxiv.org/html/2509.26633v1
47. OmniRetarget, 访问时间为 二月 3, 2026， https://omniretarget.github.io/
48. TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning - arXiv, 访问时间为 二月 3, 2026， https://arxiv.org/html/2509.11839v2
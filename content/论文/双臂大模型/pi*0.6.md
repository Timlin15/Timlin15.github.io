$\pi^*_{0.6}$是基于$\pi_{0.5}$增大vlm骨干并支持多样化的条件输出的$\pi_{0.6}$并加上强化训练而来的模型。其融合了前人的value function、advantage conditioning、offline RL、人工干预等并进行了创新性地融合得到了RECAP这个方法。

论文先总结了前人的不同rl方法的弊端，如使用PPO没法有效scale和拓展到真实世界或在预训练好的VLA模型上训练，也就是要么训练一个残差策略（训练$a_{final} = a_{vla} + \Delta a_{rl}$中的残差部分）或者干预diffusion的噪声空间来优化。在这基础上，0.6用离线rl进行端到端训练，也就是进行全量更新；并且是使用的flow base model，更具有表现力；这些通过**优势加权策略提取** (Advantage-conditioned extraction)来实现，使得RL可以scale了，因为这种方法不需要计算策略梯度。

## 概念 
> Opus 4.5 (有待细化)

在强化学习中，**策略** $\pi(a_t|o_t)$ 是核心概念，它描述了智能体根据当前观测 $o_t$（比如机器人看到的图像和关节状态）选择动作 $a_t$（比如关节角度指令）的概率分布。机器人从初始状态开始，按照策略不断做出决策，与环境交互，形成一条完整的**轨迹** $\tau = (o_0, a_0, o_1, a_1, \cdots, o_T)$。

按照策略 $\pi$ 行动会产生一个轨迹分布，其概率为：

$$\rho_\pi(\tau) = p(o_0) \prod_{t=0}^{T-1} \pi(a_t|o_t) , p(o_{t+1}|o_t, a_t)$$

其中 $p(o_0)$ 是初始状态分布，$\pi(a_t|o_t)$ 是策略选择动作的概率，$p(o_{t+1}|o_t, a_t)$ 是环境动力学（执行动作后下一个状态的概率）。

每执行一步，环境会给出一个即时**奖励** $r_t = r(o_t, a_t)$，而一条轨迹的**回报**就是所有时刻奖励的总和：

$$R(\tau) = \sum_{t=0}^{T} r_t$$

在 RECAP 的设定中，每走一步扣 1 分（鼓励快速完成），成功时终止奖励为 0，失败则扣一个很大的分数。这样回报本质上反映的就是"完成任务的速度"和"是否成功"。

强化学习的目标是找到一个策略，使得期望回报最大化：

$$J(\pi) = \mathbb{E}_{\tau \sim \rho_\pi} [R(\tau)] = \mathbb{E}_{\tau \sim \rho_\pi} \left[ \sum_{t=0}^{T} r_t \right]$$

为了评估策略的好坏，我们引入**价值函数**：

$$V^\pi(o_t) = \mathbb{E}_{\tau_{t+1:T}} \left[ \sum_{t'=t}^{T} r_{t'} \right]$$

它表示从当前状态出发、按照策略继续执行，期望还能获得多少回报。直观来说，如果 $V^\pi(o_t) = -20$，意味着预计还需要 20 步才能完成任务；如果价值是一个很大的负数，则说明当前状态很糟糕，任务很可能失败。

在价值函数的基础上，我们可以计算**优势函数**：

$$A^\pi(o_t, a_t) = \mathbb{E}_{\rho_\pi(\tau)} \left[ \sum_{t'=t}^{t+N-1} r_{t'} + V^\pi(o_{t+N}) \right] - V^\pi(o_t)$$

这是一个 N 步优势估计，衡量某个具体动作相比于平均水平好多少。等号右边第一项是执行动作 $a_t$ 后接下来 N 步的实际奖励加上 N 步后状态的价值（即"实际表现"），第二项是当前状态的价值（即"平均期望"）。两者之差就是这个动作带来的"优势"：如果为正，说明这个动作比平均水平更好；如果为负，则说明这个动作拖慢了进度或增加了失败风险。

在 RECAP 中，这些概念各司其职：VLA 模型扮演策略的角色，单独训练的价值网络用来评估状态好坏，而优势函数则用来判断每个动作的质量——优势超过阈值的动作被标记为 "positive"，反之标记为 "negative"，从而指导模型学习什么该做、什么不该做。
## **正则化强化学习**（Regularized RL）
这是 RECAP 中 advantage conditioning 方法的理论基础。

在标准 RL 中，我们直接最大化期望回报 $J(\pi)$。但在实际训练中，尤其是当我们需要在同一批数据上进行多次梯度更新时，这样做容易导致策略偏离太远、训练不稳定。因此，一个常见做法是加入**正则化项**，让新策略在提升奖励的同时，不要偏离某个参考策略 $\pi_{\text{ref}}$ 太远。这个参考策略通常就是收集训练数据的那个策略（behavior policy）。

正则化 RL 的目标函数可以写成：

$$J(\pi, \pi_{\text{ref}}) = \mathbb{E}_{\tau \sim \rho_{\pi}} \left[ \sum_{t=0}^{T} \gamma^t r_t \right] - \beta , \mathbb{E}_{o \sim \rho_{\pi}} \left[ D(\pi(\cdot|o) | \pi_{\text{ref}}(\cdot|o)) \right]$$

第一项是期望回报，第二项是新策略与参考策略之间的散度（比如 KL 散度），$\beta$ 控制正则化的强度。直观来说，这个目标函数要求策略**既要拿高分，又不能跑太偏**。

当散度 $D$ 取 KL 散度时，这个优化问题有一个著名的闭式解：

$$\hat{\pi}(a|o) \propto \pi_{\text{ref}}(a|o) \exp\left( A^{\pi_{\text{ref}}}(o, a) / \beta \right)$$

这个公式说的是：最优策略应该在参考策略的基础上，根据优势函数进行调整——优势越高的动作，概率被放大得越多。

RECAP 的方法基于一个相关但不太为人知的结论。如果我们把策略定义为：

$$\hat{\pi}(a|o) \propto \pi_{\text{ref}}(a|o) , p(I | A^{\pi_{\text{ref}}}(o, a))^\beta$$

其中 $p(I | A^{\pi_{\text{ref}}}(o, a))$ 表示动作 $a$ 能够带来改进的概率（通过一个单调递增函数 $g$ 来衡量），那么这个新策略 $\hat{\pi}$ 一定不比参考策略差，即 $J(\hat{\pi}) \geq J(\pi_{\text{ref}})$。

这个性质非常重要：它保证了只要我们能正确估计"哪些动作是好的"（即改进概率 $p(I|A)$），按照这个方式构造的新策略**一定能带来提升**。这就是 RECAP 中 advantage conditioning 的理论依据——通过给动作打上 positive/negative 标签，本质上就是在近似这个改进概率，从而引导策略向更好的方向学习。

有了这个闭式解 $\hat{\pi}$ 的形式，我们就可以通过最小化 KL 散度来训练一个参数化的策略网络 $\pi_\theta$ 去逼近它：

$$\min_\theta \mathbb{E}_{s \sim \rho_{\pi_{\text{ref}}}} \left[ \text{KL}(\hat{\pi}, \pi_\theta) \right]$$

这样就把理论上的最优策略转化成了实际可训练的神经网络。

## RECAP内容
Recap主要有一下几点内容，并且可以多次使用来提升效果：
1. 数据收集。我们在任务上运行 VLA，用任务结果标签（决定奖励）标记每个情节，并可选择提供人工干预，以提供早期迭代中错误的纠正示例。
2. 价值功能训练。我们使用迄今为止收集的所有数据来训练一个大型的多任务价值函数，我们将其称为 $V^{\pi_{ref}}$ ，它可以检测故障并判断任务完成的预期时间。
3. 条件训练的优势。为了利用该价值函数改进 VLA 策略，我们包含一个基于VLA 前缀中的值函数得出的优势值的最优性指标。这种“优势条件”配方提供了一种简单而有效的方法，可以使用次优数据从我们的价值函数中提取更最优的策略。
![image.png](https://typora-1344509263.cos.ap-guangzhou.myqcloud.com/markdown/20260123212920275.png)


### 价值函数
将价值函数视为多任务分布式价值函数$p_{\phi}(V|\mathbf{o}_t, \ell)\in \Delta_B$，即输出的是可能值的分布，将观察$\mathbf{o}$和语言指令$\ell$映射到离散化的B个值上。
模型使用$R_t(\tau) = \sum_{t'=t}^{T} r_{t'}$作为奖励函数，其中每进行一步就给-1的奖励，也就是奖励尽早结束的轨迹，如果最后成功则最终奖励为0,否则给一个绝对值很大的负数。然后将这个连续的回报值**离散化**到 $B=201$个 bin 中，得到$R_t^B$​。这样价值预测问题就变成了一个 201 类的分类问题。
这样可以用交叉熵损失：
$$
\min_\phi \mathbb{E}_{\tau \in \mathcal{D}} \left[ \sum_{o_t \in \tau} H(R_t^B(\tau), p_\phi(V|o_t, \ell)) \right]
$$
来训练价值函数。最后由
$$
V^{\pi_{\text{ref}}}(o_t, \ell) = \sum_{b \in [0, B]} p_\phi(V = b | o_t) \cdot v(b)
$$
提取出连续的价值函数。
论文指出，这是一个 **Monte Carlo 估计器**，即直接用轨迹的实际回报作为监督信号。这种方法的优缺点是：
- 优点在于简单可靠。不需要像 Q-learning 那样做 bootstrapping（用下一个状态的预测值来更新当前预测值），避免了值函数估计的累积误差。
- 缺点在于是 on-policy 的。它只能准确估计收集数据的那个策略（behavior policy πref\pi_{\text{ref}} πref​）的价值，而不是某个假想的最优策略的价值。
但论文认为这个缺点可以接受，因为 RECAP 的目标不是找到全局最优策略，而是**相对于当前策略进行改进**。只要能区分出哪些动作比当前策略的平均水平好，就足以指导策略更新了。

### 如何利用价值函数来改进策略。
还没有看懂的理论基础，留着以后看
**问题背景**

有了价值函数 $V^{\pi_{\text{ref}}}$ 之后，我们需要一种方法把它转化为更好的策略。这个过程叫做**策略提取**（policy extraction）。一个好的策略提取方法需要满足几个条件：能有效利用各种来源的离线数据（演示、自主探索、人工干预），能适用于使用 flow matching 的大规模 VLA 模型，以及能同时从好数据和坏数据中学习。

传统的策略梯度方法（如 PPO）需要计算策略的对数概率，但 flow matching 模型很难提供这个，因此不太适用。加权回归方法（如 AWR）会丢弃大量低 advantage 的数据，造成浪费。RECAP 采用的是 **advantage conditioning**，把 RL 问题转化为条件生成问题。

**改进概率的定义**

RECAP 的方法基于正则化 RL 的理论。论文定义了一个"改进概率"：

$$p(I | A^{\pi_{\text{ref}}}(o, a)) = \delta(A^{\pi_{\text{ref}}}(o, a) > \epsilon_\ell)$$

这是一个 delta 分布，意思很简单：如果动作的 advantage 超过阈值 $\epsilon_\ell$，改进概率为 1（positive）；否则为 0（negative）。阈值 $\epsilon_\ell$ 是任务相关的，设为该任务所有数据 advantage 的第 30% 分位数。

根据理论，如果按照 $\hat{\pi}(a|o) \propto \pi_{\text{ref}}(a|o) , p(I|A)^\beta$ 构造新策略，可以保证 $J(\hat{\pi}) \geq J(\pi_{\text{ref}})$，即新策略一定不比原策略差。

**训练目标**

基于上述理论，可以通过贝叶斯公式将改进概率重写为 $p(I|A) = \pi_{\text{ref}}(a|I, o) / \pi_{\text{ref}}(a|o)$。当 $\beta = 1$ 时，最优策略简化为 $\hat{\pi}(a|o) = \pi_{\text{ref}}(a|I, o)$，即给定"positive"标签时的条件分布。

因此，VLA 的训练目标变成：

$$\min_\theta \mathbb{E}_{\mathcal{D}} \left[ -\log \pi_\theta(a_t | o_t, \ell) - \alpha \log \pi_\theta(a_t | I_t, o_t, \ell) \right]$$

其中 $I_t = \mathbb{1}(A^{\pi_{\text{ref}}}(o_t, a_t, \ell) > \epsilon_\ell)$。

第一项是标准的行为克隆损失，第二项是 advantage-conditioned 损失。模型通过第二项学会了将 positive 标签与高 advantage 动作关联、negative 标签与低 advantage 动作关联。

**实际实现**

在实现上，advantage 标签被转换为文本输入 "Advantage: positive" 或 "Advantage: negative" 加入 prompt。训练时随机丢弃标签（类似 classifier-free guidance），让模型同时学会有条件和无条件生成。

对于人工干预的数据，直接强制设置 $I_t = \text{True}$，因为假设人类专家的纠正动作总是好的。

推理时只输入 "Advantage: positive"，模型就会输出它学到的高 advantage 动作，实现策略改进。如果需要更强的引导效果，可以用 $\beta > 1$ 结合 classifier-free guidance。

**为什么这种方法有效**

这种方法的优势在于：它把复杂的 RL 优化问题转化为简单的条件生成问题，价值函数负责"打分"，VLA 只需学会"根据分数标签生成对应质量的动作"。这种解耦使得训练更稳定，也能充分利用所有数据（好的和坏的都用上）。与 PPO 等方法相比，它不需要计算 flow matching 模型的对数概率，更适合大规模 VLA 训练。

## 模型细节

为了防止灾难性遗忘，模型训练采用知识隔离(KI)的方法训练。VLA 模型需要同时具备两种能力：一是视觉语言理解能力（理解图像、理解指令），二是动作生成能力（输出精确的机器人控制指令）。这两种能力的训练数据和训练目标差异很大。视觉语言能力来自海量的网络图文数据，用 next-token prediction 训练。动作生成能力来自机器人演示数据，用 flow matching 训练连续动作。

如果简单地端到端联合训练，会出现**能力冲突**：动作生成的梯度会"污染"视觉语言主干网络，导致模型忘记之前学到的视觉语言知识。
知识隔离使用了stop gradiant使得训练动作专家更新的梯度不会回传到vlm模型中，防止了动作专家影响整个网络同时保证了端到端的训练效果。

$\pi_{0.6}$比$\pi_{0.5}$在这几个方面改善了：
1. 使用了更多的机器人平台数据
2. 使用了4B的gemma4模型
3. 扩大了动作专家的模型大小
剩余的训练基本和$\pi_{0.5}$一致

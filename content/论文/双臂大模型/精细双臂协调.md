[Morphologically Symmetric Reinforcement Learning for Ambidextrous Bimanual Manipulation | PDF](https://arxiv.org/pdf/2505.05287)

## 论文动机
论文想模仿人类的双手合作的案例，比如可以在简单的事上轻易地对称左右手所做的事但是在复杂的事情上可以专注于一只手的精细动作。先前的研究基本都是研究机器人腿部而少有研究手部的对称。

同时，通过RL进行双臂训练是一个很大的挑战：
- 高维观测和动作空间使得策略学习困难
- 奖励函数可能导致双臂不能同时获得最佳策略
- 精细的双手动作相当于多任务学习，虽然可以为每个子任务定义一个奖励函数但是调参复杂
- RL为sim-to-real的gap和安全问题提出挑战
## 论文方法
针对以上问题，论文提出了一个双臂灵巧学习的强化学习框架**SYMDEX**(SYMmetric DEXterity)，并总结了以下创新点：
- 形态上包含对称的学习方式
- 可扩大泛化的多臂学习方案（通过把复杂精细任务拆成子任务分别进行强化学习后合成全局策略）
- 完整的sim-to-real转化

论文用马可夫决策过程(MDP)对系统进行建模，并用图论赋予此系统对称性。
论文用POMDP定义了一个元组：
$$\mathcal{P} = (S, \mathcal{A}, \tau, r, \mathcal{O}, \sigma, \gamma) $$
- $S, \mathcal{A}, \tau, r, \gamma$: 和 MDP 相同 
- $\mathcal{O}$: 观测空间 (observation space) 
- $\sigma(o|s,a)$: 观测概率模型 (observation model)，即在状态 $s$ 下执行动作 $a$，智能体得到观测 $o$ 的概率 
关键区别: 
- 在 POMDP 中，智能体 **不能直接看到** $s$，只能通过 $\sigma$ 得到一个观测 $o$。 
- 因此智能体需要维护一个 **信念状态 (belief state)**：即对真实状态 $s$ 的概率分布 $b(s)$，并根据贝叶斯法则不断更新。

同时利用图论的等变形和不变性使POMDP具有了对称性。
**定义 A.5（$\mathbb{G}$-等变和 $\mathbb{G}$-不变映射）**。设 $\mathcal{X}$ 和 $\mathcal{Y}$ 是具有相同对称群 $\mathbb{G}$ 的两个向量空间，分别带有群作用 $\triangleright_{\mathcal{X}}$ 和 $\triangleright_{\mathcal{Y}}$。一个映射 $f: \mathcal{X} \mapsto \mathcal{Y}$ 被称为 $\mathbb{G}$-等变的，如果它与群作用可交换，使得： $$ \begin{aligned} g \triangleright_{\mathcal{Y}} \mathbf{y} &= g \triangleright_{\mathcal{Y}} f(\mathbf{x}) = f(g \triangleright_{\mathcal{X}} \mathbf{x}), \quad \forall \mathbf{x} \in \mathcal{X}, g \in \mathbb{G}. \\ \rho_{\mathcal{Y}}(g) f(\mathbf{x}) &= f(\rho_{\mathcal{X}}(g) \mathbf{x}) \end{aligned} \quad \iff \quad \begin{aligned} \mathcal{X} &\xrightarrow{\triangleright_{\mathcal{X}}} \mathcal{X} \\ &\downarrow f \qquad \downarrow f \\ \mathcal{Y} &\xrightarrow{\triangleright_{\mathcal{Y}}} \mathcal{Y} \end{aligned} $$
$\mathbb{G}$-等变映射的一个特例是 $\mathbb{G}$-不变映射，它们与群作用可交换，并且具有平凡的输出群作用 $\triangleright_{\mathcal{Y}}$，使得 $\rho_{\mathcal{Y}}(g) = \mathbf{I}$ 对所有 $g \in \mathbb{G}$ 成立。即： $$ \begin{aligned} \mathbf{y} &= g \triangleright_{\mathcal{Y}} f(\mathbf{x}) = f(g \triangleright_{\mathcal{X}} \mathbf{x}), \quad \forall \mathbf{x} \in \mathcal{X}, g \in \mathbb{G}. \\ \mathbf{y} &= \rho_{\mathcal{Y}}(g) f(\mathbf{x}) = f(\rho_{\mathcal{X}}(g) \mathbf{x}) \end{aligned} \quad \iff \quad \begin{aligned} \mathcal{X} &\xrightarrow{\triangleright_{\mathcal{X}}} \mathcal{X} \\ &\searrow f \qquad \downarrow f \\ &\mathcal{Y} \xrightarrow{\triangleright_{\mathcal{Y}}} \mathcal{Y} \end{aligned} $$
将这个性质代入POMDP中：
**定义 B.1（对称 POMDP）**。一个 POMDP $(\mathcal{S}, \mathcal{A}, r, \tau, \rho_0, \gamma, \mathcal{O}, \sigma)$ 具有对称群 $\mathbb{G}$，当状态空间 $\mathcal{S}$ 和动作空间 $\mathcal{A}$ 承认群作用 $(\triangleright_{\mathcal{S}})$ 和 $(\triangleright_{\mathcal{A}})$，且 $(r, \tau, \rho_0)$ (奖励函数)都是 $\mathbb{G}$-不变的。也就是说，对于每一个 $g \in \mathbb{G}$，$s, s' \in \mathcal{S}$，和 $a \in \mathcal{A}$，我们有： $$ \begin{aligned} \tau(g \triangleright_{\mathcal{S}} s' \mid g \triangleright_{\mathcal{S}} s, g \triangleright_{\mathcal{A}} a) &= \tau(s' \mid s, a), \\ \rho_0(g \triangleright_{\mathcal{S}} s) &= \rho_0(s), \\ r(g \triangleright_{\mathcal{S}} s, g \triangleright_{\mathcal{A}} a) &= r(s, a). \end{aligned} $$ 满足方程 (2) 的 POMDP 被约束为具有最优策略和价值函数，它们满足： $$ \begin{aligned} \underbrace{g \triangleright_{\mathcal{A}} \pi^*(\sigma(s))}_{\text{Policy } \mathbb{G}\text{-equivariance}} &= \pi^*(\sigma(g \triangleright_{\mathcal{S}} s)), \\ \underbrace{V^*(\sigma(s))}_{\text{Value function } \mathbb{G}\text{-invariance}} &= V^*(\sigma(g \triangleright_{\mathcal{S}} s)), \end{aligned} \quad \forall s \in \mathcal{S}, \; g \in \mathbb{G}. \quad (\text{参见 [20]}) $$
也就是利用这个性质，可以给让双臂机器人的任务和动作进行对调，比如论文中举出了一个打蛋的操作，分为拿着碗和打蛋两个子动作，通过群动作$g_r\triangleright_1$来对agent-task对进行变换：
$$ g_r \triangleright_{\mathcal{P}} \left[\begin{pmatrix} \mathbf{L}, & \mathbf{B} \\ \mathbf{R}, & \mathbf{E} \end{pmatrix} \right]= \left[\begin{pmatrix} \mathbf{L}, & g_r \triangleright_{\mathcal{K}} \mathbf{B} \\ \mathbf{R}, & g_r \triangleright_{\mathcal{K}} \mathbf{E} \end{pmatrix}\right] = \begin{bmatrix} \mathbf{L}, & \mathbf{E} \\ \mathbf{R}, & \mathbf{B} \end{bmatrix}. $$
$$
g_r \triangleright_{\mathcal{A}} \mathbf{a} := g_r \triangleright_{\mathcal{A}} \begin{bmatrix}
\mathbf{a}^{\text{L}} \sim \pi_{\text{B}}(\mathbf{o}^{\text{L,B}}) \\
\mathbf{a}^{\text{R}} \sim \pi_{\text{E}}(\mathbf{o}^{\text{R,E}})
\end{bmatrix} = \begin{bmatrix}
\mathbf{a}^{\text{L}} \sim g_r \triangleright_{\mathcal{A}_{\text{B}}} \pi_{\text{B}}(\mathbf{o}^{\text{L,B}}) \\
\mathbf{a}^{\text{R}} \sim g_r \triangleright_{\mathcal{A}_{\text{E}}} \pi_{\text{E}}(\mathbf{o}^{\text{R,E}})
\end{bmatrix} = \begin{bmatrix}
\mathbf{a}^{\text{L}} \sim \pi_{\text{E}}(\sigma^{\text{L}}(g_r \triangleright_{\mathcal{S}} s, \mathbf{E})) \\
\mathbf{a}^{\text{R}} \sim \pi_{\text{B}}(\sigma^{\text{R}}(g_r \triangleright_{\mathcal{S}} s, \mathbf{B}))
\end{bmatrix}
$$
这个公式表示对原始动作进行群对称化操作$g_r\triangleright_{\mathcal{A}}$最后可以得到在另外一个臂上的观测空间的对称$\sigma^L(g_r\triangleright_S s, E)$，同时对等变的任务策略和观测函数进行变化，导致镜像后的左臂的动作为原环境中右臂动作的对称版本，反之亦然。

同时这个操作适配多agent。

另外一个创新点是论文中提出的等变神经网络，使得策略学习具有对称性
$$ g \triangleright_{\mathcal{A}_{\theta_k}} \pi_k^{\theta_k}(\mathbf{o}^{n,k}) = \pi_k^{\theta_k}(g \triangleright_{\mathcal{O}_k} \sigma^n(s, k)) = \pi_k^{\theta_k}(\sigma^{(g \triangleright_{\mathbb{I}} [k])}(g \triangleright_{\mathcal{S}} s, k)), \quad \forall (n, k) \in \mathbb{I}, g \in \mathbb{G}. $$
其中$\pi_k^{\theta_k}$就是等变神经网络，$\theta_k$是神经网络的参数。
满足
- **置换 agent 轴**（$n_1\leftrightarrow n_2​$）
- **置换子任务轴**（$k_1\leftrightarrow k_2$）
- **对几何通道做符号翻转/通道交换**
后输出结果完成相应变化
![image.png](https://typora-1344509263.cos.ap-guangzhou.myqcloud.com/test/20250921160858.png)

最后对策略遵从teacher-student范式进行蒸馏，获得全局参数。
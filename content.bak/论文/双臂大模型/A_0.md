[A0: An Affordance-Aware Hierarchical Model for General Robotic Manipulation | PDF](https://arxiv.org/pdf/2504.12636)
## 论文动机
现有的模块化设计和端到端设计难以掌握空间推理能力，不同于现有的基于点和flow的模型，A0利用了Embodiment-Agnostic Affordance Representation（一种不依赖于机器人形态，捕捉物体与任务相关的功能点和交互可能性），基于预测接触点(contact point)以及后接触点(post-contact point)
## 论文方法
论文主要强调几点：
- 空间推理能力
- 有等级的模型
- 计算效率性
先从前一个点来讲，同[[H-RDT概述]]，本论文也强调不能只从机器人数据中获取数据，而是要从互联网级别的HOI(hand-object interaction)素材中学习，认为其中富含物体互动，空间特性，物理特性等信息，不同的是，H-RDT强调egocentric，本论文强调物体为中心，捕捉物体的特性。
同时，与只训练预测轨迹的流模型等，论文通过捕捉接触点和后接触点来捕捉物体性质，使得其与载体无关。
**相关工作**：
- 空间推理能力
	前线的空间表示能力主要分为三类：热力图；Bounding box； keypoint。本论文基于并改进了keypoint方法
- 等级模型
	近期在VLA领域的进步分为基于Transformer的；基于VLM模型的；基于diffusion模型的。
	三者各有优劣。Transformer用视频生成模型预测动作串；VLM模型利用预训练的视频语言模型来增强泛化性；Diffusion基于概率动作生成有利于健壮的轨迹生成。
	并且不同于VLA同长仅在一个具身上有效果，A0利用的Diffusion骨架模型有效

A0采用了：
- 高阶的空间理解模型
- 低阶动作执行模型
对于空间理解，A0利用了Diffusion模型来进行物品预设功能学习。
对于数据$\mathcal{R}$，可有真实/合成数据$\mathcal{R}_R$，HOI数据$\mathcal{R}_H$，以及自定义数据$\mathcal{R}_C$。数据包括维度为$2D$的RGB图像，接触点$c_0^{2D}$以及后接触点轨迹$T=(t_0^{2D},t_1^{2D},\cdots)$。因此，被建设的数据集可以如此表示：
$$
\begin{aligned}
\mathcal{R} &= \mathcal{R}_R \cup \mathcal{R}_H \cup \mathcal{R}_C \\
&=\{(I,L,C,T)\,|\,C=(c_0^{2D}),\, T=(t_0^{2D},t_1^{2D},\cdots) \}
\end{aligned}
$$
通过这个数据表示形式，就可以得到与训练目标契合度高的数据，就可以用少量的下游数据标注图像进行fine tuning，便于快速的跨平台布置。
只集中于计算接触点和轨迹使得模型具有计算效率性。
![image.png](https://typora-1344509263.cos.ap-guangzhou.myqcloud.com/test/20250929162137.png)
A0中的Diffusion模型作为高阶模型，Action Expert作为动作模型。
A0输出多个接触点：
$$\mathbf{x}_{t:t+T}, 其中\mathbf{x}_t = (u,v)\in[0,1]^2\in R^2 $$
来表示第在轨迹中的t个时间点，第一个点是起始点。
作为输入Diffusion模型的变量，A0接受噪声和时间步k。其中噪音通过一个MLP获得，时间步k是由一个embedder转为向量。这样Diffusion就可以在处理轨迹token的时候感知现在是第几步去噪。

同时A0接受图像和文字encoder的输入。都是使用已经训练好的模型接入。其中由于单帧的图画不能很好得表示变化的信息，难以获取动态的affordance，所以模型采用输入上下帧的方式，通过相减获得差分信息，再和当前帧拼接，作为Diffusion的输入：
$$I_i^m = I_t^i - I^i_{t-1}, \quad o_t = concat([I_t^i, I^i_m,dim=1]) $$
同时，通过在视觉的空间中添加与time step相关的正弦位置embedding来增强模型对viewpoint和time steps的区分能力。之后，这个语言和图像token再进入一个交叉注意力来压缩数据维度，应该还能同时强调图片关于语言提及的部分。

**关于Diffusion部分不熟，可能近期系统地了解了一下再补充**

**训练部分**
![image.png](https://typora-1344509263.cos.ap-guangzhou.myqcloud.com/test/20250929171018.png)
模型训练分为预训练和监督微调(Supervised fine tune, SFT)
PT: 由于模型只预测接触点和轨迹，所以模型与训练的时候并不依赖机器人的或者人的egocentric视频，而是可以从互联网级别的视频中学习物体的性质。通过对这些数据生成的接触点和轨迹与ground-truth接触点和轨迹比较，并最小化轨迹差以及更重要的初始点：
$$ \mathcal{L}_p(\theta) = \frac{1}{n} \sum_{i=1}^{n} \left( (x_t^0)_i - (f_\theta(k, x_t^k, I_t, \ell))_i \right)^2. $$
SFT: 也包含一部分Diffusion模型，考虑之后再看。

最后使用一个Action System使得模型具有跨具身能力。
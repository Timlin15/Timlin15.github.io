ref:[CS229笔记]([main_notes.pdf](https://cs229.stanford.edu/main_notes.pdf))第15章和[中文翻译仓库](https://github.com/Na-moe/CS229_CN/blob/main)
## 马可夫决策过程(MDP)
一个马可夫决策过程由一个元组定义$(S,A, \{P_{SA}\},\gamma, R)$，其中：
- $S$:是一系列状态
- $A$:是一系列动作
- $P_{SA}$:是状态转换可能，对于$s\in S, a\in A$，$P_{SA}$是当下状态所有可能转换的状态的可能性
- $\gamma\in [0,1)$:是discount factor
- $R:S\times A\mapsto R$:是奖励函数
MDP的动态过程如下：我们从某个状态$s_0$开始，然后在MDP中选择一个动作 $a_0 ∈ A$ 执行。然后 MDP 的状态随机转移到某个后继状态$s_1$，根据$s_1 ∼P_{s_0a_0}$ 抽取。然后选择另一个动作$a_1$。由于这个动作，状态再次转移，现在转移到某个 $s_2 ∼ P_{s_1a_1}$ ，依此类推。可以将这个过程表示为：
$$
s_0 \xrightarrow{a_0} s_1 \xrightarrow{a_1} s_2 \xrightarrow{a_2} s_3 \xrightarrow{a_3} \dots
$$
以动作序列 $a_0, a_1, \dots$ 遍历状态序列 $s_0, s_1, \dots$ 后，总收益由下式给出
$$
    R(s_0, a_0) + \gamma R(s_1, a_1) + \gamma^2 R(s_2, a_2) + \dots.
$$
或者，将奖励写成仅关于状态的函数时，则变为
$$
    R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \dots.
$$
在强化学习中，目标是随着时间推移选择动作以最大化总收益的期望值：
$$
    \text{E}\left[R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \dots\right]
$$
**策略 (policy)** 是一个函数 $\pi: S \mapsto A$，它将状态映射到动作。当处于状态 $s$ 时，如果执行 (executing) 某个策略 $\pi$，则采取动作 $a = \pi(s)$。同时定义策略 $\pi$ 的价值函数 (value function) 为
$$
    V^\pi(s) = \text{E}\left[R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \dots \mid s_0 = s, \pi\right].
$$
$V^\pi(s)$ 表示从状态 $s$ 开始并按照策略 $\pi$ 采取动作所获得的折扣奖励的期望总和。\footnote{请注意，这里以 $\pi$ 为条件的写法并不完全正确，因为 $\pi$ 不是随机变量，但这在文献中是相当标准的用法。

给定一个固定的策略 $\pi$，其价值函数 $V^\pi$ 满足贝尔曼方程 (Bellman equation)：
$$
    V^\pi(s) = R(s) + \gamma \sum_{s' \in S} P_{s\pi(s)}(s') V^\pi(s').
$$
这表明从状态 $s$ 开始的折扣奖励期望总和 $V^\pi(s)$ 由两部分组成：第一部分是从状态 $s$ 开始即刻获得的即时奖励 (immediate reward) $R(s)$；第二部分是未来折扣奖励的期望总和。仔细考察第二项，可以看到上面的求和项可以重写为 $\text{E}_{s' \sim P_{s\pi(s)}}[V^\pi(s')]$。这是从状态 $s'$ 开始的折扣奖励的期望总和，其中 $s'$ 的分布由 $P_{s\pi(s)}$ 给出，也就是在 MDP 中从状态 $s$ 执行第一个动作 $\pi(s)$ 后将到达的状态分布。因此，上面的第二项给出的是在 MDP 中执行第一步后获得的折扣奖励的期望总和。

贝尔曼方程可以有效地用于求解 $V^\pi$。具体来说，在一个有限状态 MDP ($|S| < \infty$) 中，可以为每个状态 $s$ 写出一个关于 $V^\pi(s)$ 的方程。这给出了 $|S|$ 个线性方程组，其中包含 $|S|$ 个变量（未知的 $V^\pi(s)$），可以有效地求解这些变量。

同样地，定义最优价值函数 (optimal value function)为
$$
    V^*(s) = \max_{\pi} V^\pi(s).
$$
换句话说，这是使用任何策略可以达到的最佳期望折扣奖励总和。对于最优价值函数，也有一个贝尔曼方程：
$$
    V^*(s) = R(s) + \max_{a \in A} \gamma \sum_{s' \in S} P_{sa}(s') V^*(s').
$$
上面的第一项是即时奖励。第二项是在执行动作 $a$ 之后获得的期望未来折扣奖励总和在所有动作 $a$ 上的最大值。应该确保理解这个方程及其合理性。
同时定义策略 $\pi^*: S \mapsto A$ 如下：
$$
    \pi^*(s) = \arg \max_{a \in A} \sum_{s' \in S} P_{sa}(s') V^*(s').
$$
注意，$\pi^*(s)$ 给出了在方程eq:15.2中的 "max" 中达到最大值的动作 $a$。

事实证明，对于每一个状态 $s$ 和每一个策略 $\pi$，有
$$
    V^*(s) = V^{\pi^*}(s) \ge V^\pi(s).
$$
第一个等号表示，对于每个状态 $s$，策略 $\pi^*$ 的价值函数 $V^{\pi^*}$ 都等于最优价值函数 $V^*$。此外，不等号表示 $\pi^*$ 的价值至少与任何其他策略的价值一样大。换句话说，方程所定义的 $\pi^*$ 是最优策略 (optimal policy)。


## MARL
在多个智能体的情况下进行强化学习，MDP和policy gradiant都不能很好胜任，因此[Trust Region Policy Optimisation in Multi-Agent Reinforcement Learning | PDF](https://arxiv.org/pdf/2109.11251)提出HATRPO/HAPPO训练方式。同时参考了Numerical Optimization

**状态价值函数 $V_\pi(s)$**

**定义：**
$$
V_\pi(s) = \mathbb{E}\Bigg[ \sum_{t=0}^\infty \gamma^t r_t \;\Big|\; s_0 = s,\, a_t \sim \pi \Bigg]
$$
**含义：**  
在初始状态为 $s$ 的条件下，遵循策略 $\pi$ 时，期望获得的 **累计折扣回报**。用来衡量某个状态本身的“好坏”。

**状态-动作价值函数 $Q_\pi(s,a)$**

**定义：**

$$Q_\pi(s,a) = \mathbb{E}\Bigg[ \sum_{t=0}^\infty \gamma^t r_t \;\Big|\; s_0 = s,\, a_0 = a,\, a_{1:\infty} \sim \pi \Bigg]$$

**含义：**  
在状态 $s$下先执行动作 $a$，之后遵循策略 $\pi$，期望获得的 **累计折扣回报**。用来衡量某个动作在某个状态下的“好坏”。

**策略目标函数 $J(\pi)$**

**定义：**

$$J(\pi) = \mathbb{E}_{s_0 \sim \rho_0}\Big[ V_\pi(s_0) \Big] = \mathbb{E}_{s_0:\infty \sim \rho_\pi,\, a_0:\infty \sim \pi}\Bigg[ \sum_{t=0}^\infty \gamma^t r_t \Bigg]$$

**含义：**  
策略 $\pi$ 在 **整个初始状态分布 $\rho_0$** 下的期望累计回报，是强化学习中需要最大化的最终目标。
因此，我们的目标是最大化这个函数：(假定所有智能体共享同样的奖励函数)
$$
J(\pi) \triangleq \mathbb{E}_{s_{0:\infty} \sim \rho_{\pi}^{0:\infty}, a_{0:\infty} \sim \pi} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \right].$$
同时定义$A$优势函数，对于单智能体和多智能体，定义分别为：
$$
A_{\pi}(s,a) = Q_{\pi}(s,a)-V_{\pi}(s)
$$
$$
A_{\pi}^{i1:m}\left(s, \mathbf{a}^{j1:k}, \mathbf{a}^{i1:m}\right) \triangleq Q_{\pi}^{j1:k,i1:m}\left(s, \mathbf{a}^{j1:k}, \mathbf{a}^{i1:m}\right) - Q_{\pi}^{j1:k}\left(s, \mathbf{a}^{j1:k}\right).$$
### 信赖域算法
信赖域方法是一类用于数值优化的迭代算法。它的核心思想是：  
- 在每一步迭代时，不是直接在全局搜索下降方向，而是在当前点附近建立一个**局部近似模型**（通常是二次模型），然后只在一个“可信”的区域（trust region）内对这个近似模型进行优化。
定理： 设 $\pi$ 是当前策略，$\bar{\pi}$ 是下一个候选策略。我们定义 $L_\pi(\bar{\pi}) = J(\pi) + \mathbb{E}_{s\sim\rho_\pi, a\sim\bar{\pi}} [A_\pi(s, a)]$， $D^{max}_{KL}(\pi, \bar{\pi}) = \max_s D_{KL}(\pi(\cdot|s), \bar{\pi}(\cdot|s))$。$L$是用上一步的策略来构造近似下一步的奖励函数；$D$ 指 **KL 散度**（常写 $D_{\mathrm{KL}}$​），用来度量**新旧策略的分布差**，在信赖域法里作为**约束/惩罚**控制更新“别走太远”。
那么以下不等式成立： $$J(\bar{\pi}) \geq L_\pi(\bar{\pi}) - C D^{max}_{KL}(\pi, \bar{\pi})$$ 其中 $$C = \frac{4\gamma \max_{s,a} |A_\pi(s,a)|}{(1-\gamma)^2}$$
所以当当前策略$\pi$和下一步策略$\bar{\pi}$距离很近的时候，只根据上一步推断出来的$L_{\pi}(\bar{\pi})$会和$J(\bar{\pi})$非常接近。所以agent可以通过信赖域来迭代其策略：
$$
π_{k+1} = \arg \max_{\pi} \left( L_{\pi_k}(\pi) - C \mathbf{D}_{KL}^{\max}(\pi_k, \pi) \right).
$$
但是这种方法并不实用，计算困难，论文提出TRPO算法，即：
$$θ_{k+1} = \arg \max_{θ} L_{π_{θ_k}}(π_θ), \quad \text{subject to } \mathbb{E}_{s \sim ρ_{π_{θ_k}}} \left[ D_{KL}(π_{θ_k}, π_θ) \right] ≤ δ.$$
每一次迭代，TRPO在策略$\pi_{\theta_k}$构建一个KL球$\mathcal{B}_\delta(\pi_{\theta_k})$，使得$L{\pi_{\theta_k}}(\pi_{\theta})$和真实奖励函数$J(\pi_{\theta})$相近。为了减轻计算散度的期望的计算负担，论文提出了PPO算法：
$$L_{π_{θ_k}}^{PPO}(π_θ) = \mathbb{E}_{s \sim ρ_{π_{θ_k}}, a \sim π_{θ_k}} \left[ \min \left( \frac{π_θ(a|s)}{π_{θ_k}(a|s)} A_{π_{θ_k}}(s, a), \text{clip} \left( \frac{π_θ(a|s)}{π_{θ_k}(a|s)}, 1 - ε, 1 + ε \right) A_{π_{θ_k}}(s, a) \right) \right].$$
- $r_\theta(s,a)=\frac{π_θ(a|s)}{π_{θ_k}(a|s)} A_{π_{θ_k}}(s, a)$：**策略比**（新/旧策略在同一 $(s,a)$ 上的相对概率）。
- $A_{\pi_{\theta_k}}(s,a)$：**优势函数**（常用 GAE 估计）。
- $\mathrm{clip}(r,1\!\pm\!\epsilon)=\min(\max(r,1-\epsilon),1+\epsilon)$：把 $r$限制在 $[1-\epsilon,1+\epsilon]$。
- 外层 $\min(\cdot,\cdot)$：在“未裁剪值”和“裁剪后值”之间取**更保守**的那个，避免过度乐观。

- $A>0$（动作优于平均）：希望 **增大** 其概率（$r\uparrow$）。若 r>1+ϵr>1+\epsilon，被截断为 $(1+\epsilon)A$，通过 $⁡\min$ **限制上涨幅度**。
- $A<0$（动作劣于平均）：希望 **降低** 其概率（$r\downarrow$）。若 $r<1-\epsilon$，被截断为 $(1-\epsilon)A$，通过 $\min$ **限制下跌幅度**。
### 信赖域算法在MARL中的应用
一种原始的应用方法是直接共享参数，用聚合轨迹进行策略训练，这个方法由MAPPO提出：
$$L_{π_{θ_k}}^{MAPPO}(π_θ) = \sum_{i=1}^{n} \mathbb{E}_{s \sim ρ_{π_{θ_k}}, \mathbf{a} \sim π_{θ_k}} \left[ \min \left( \frac{π_θ(\mathbf{a}^i|s)}{π_{θ_k}(\mathbf{a}^i|s)} A_{π_{θ_k}}(s, \mathbf{a}), \text{clip} \left( \frac{π_θ(\mathbf{a}^i|s)}{π_{θ_k}(\mathbf{a}^i|s)}, 1 - ε, 1 + ε \right) A_{π_{θ_k}}(s, \mathbf{a}) \right) \right].$$
但是MAPPO有致命的缺陷：参数共享决定了智能体只能有相同的action space，可能导致并不能找到最优策略。因此论文提出可以使用HAPPO和HATRPO算法。

**多智能体的优势函数**
在任何一个合作马可夫游戏中，给定一个联合策略$\pi$，对于任何状态$s$，以及任何智能体子集$i_{1:m}$，定义如下方程：
$$
\begin{aligned}
A_{π}^{i1:m}(s, a^{i1:m}) &= \sum_{j=1}^{m} A_{π}^{ij}(s, a^{i1:j-1}, a^{ij}).\\
A_{π}^{ij}(s, a^{i1:j-1}, a^{ij}) &= Q_{π}(s, \mathbf{a}(\{i1:j\})) - Q_{π}(s, \mathbf{a}(\{i1:j-1\})).
\end{aligned}
$$
- 求和符号右侧式子表示**一组代理 $i_{1:m}$​** 同时把动作从“旧策略的基线动作”换成给定的新动作 $a^{i_{1:m}}$​ 时产生的**联合优势**（对旧策略 $\pi$ 而言）。
- 等式右侧表示前 $j-1$ 个代理已用新动作 $a^{i_{1:j-1}}$​，再让第 $j$ 个代理把动作改为 $a^{i_j}$​ 所带来的**边际优势**；把这些边际优势从 $j=1$ 到 $m$ 加起来，恰好等于“所有人一起改”的联合优势。
设 π 是一个联合策略，$\bar{\pi}^{i1:m-1} = \prod_{j=1}^{m-1} \bar{\pi}^{ij}$ 是其他代理 $i_{1:m-1}$ 的某个其他联合策略，而 $\hat{\pi}^{im}$ 是代理 $i_m$ 的某个其他策略。那么 
$$L_{\pi}^{i1:m}\left(\bar{\pi}^{i1:m-1}, \hat{\pi}^{im}\right) \triangleq \mathbb{E}_{s \sim \rho_{\pi}, \mathbf{a}^{i1:m-1} \sim \bar{\pi}^{i1:m-1}, a^{im} \sim \hat{\pi}^{im}}\left[A_{\pi}^{im}\left(s, \mathbf{a}^{i1:m-1}, a^{im}\right)\right]$$
请注意，对于任何 $\bar{\pi}^{i1:m-1}$，我们有 
$$
\begin{aligned}
L_{\pi}^{i1:m}\left(\bar{\pi}^{i1:m-1}, \pi^{im}\right) &= \mathbb{E}_{s \sim \rho_{\pi}, \mathbf{a}^{i1:m-1} \sim \bar{\pi}^{i1:m-1}, a^{im} \sim \pi^{im}}\left[A_{\pi}^{im}\left(s, \mathbf{a}^{i1:m-1}, a^{im}\right)\right]\\
&= \mathbb{E}_{s \sim \rho_{\pi}, \mathbf{a}^{i1:m-1} \sim \bar{\pi}^{i1:m-1}}\left[\mathbb{E}_{a^{im} \sim \pi^{im}}\left[A_{\pi}^{im}\left(s, \mathbf{a}^{i1:m-1}, a^{im}\right)\right]\right] = 0
\end{aligned}
$$
- 含义：在旧策略的状态分布 $π\rho_\pi$​ 下，让前 $m−1$ 个代理按 $\bar\pi$ 出动作，第 $i_m$​ 个代理按 $\hat\pi$ 出动作，计算“**第 $i_m$​ 个代理的边际优势**”的期望。它是一个**局部/代理目标**，衡量“把第 $i_m$​ 个体从旧策略换成 $\hat\pi$ 的收益”，条件是其他体用 $\bar\pi$。即用每个智能体的更新策略的优势函数加和表示代理函数（在常见实践中）。
![image.png](https://typora-1344509263.cos.ap-guangzhou.myqcloud.com/test/20250920162012.png)
### HATRPO/HAPPO
算法1使用的是散度$D_{KL}^{max}$，难估计且不光滑。同TRPO中的方法，将这个约束转为
$$
\mathbb{E}_{s \sim \rho_{π_{θ_k}}} \left[ D_{KL}(\pi_{θ_k}^{i_m}(·|s) \| π_{θ}^{i_m}(·|s)) \right] ≤ δ.$$
最后的目标变成了求以下这个目标的最大值：
$$
\begin{aligned}
θ_{k+1}^{i_m} &= \arg \max_{θ^{i_m}} \mathbb{E}_{s \sim ρ_{π_{θ_k}}, \mathbf{a}^{i1:m-1} \sim π^{i1:m-1}, a^{im} \sim π_{θ^{i_m}}^{im}} \left[ A_{π_{θ_k}}^{im}(s, \mathbf{a}^{i1:m-1}, a^{im}) \right], \\&subject\,\, to\,\, \mathbb{E}_{s \sim ρ_{π_{θ_k}}} \left[ D_{KL}(π_{θ_k}^{im}(·|s) \| π_{θ^{i_m}}^{im}(·|s)) \right] ≤ δ.
\end{aligned}
$$
然后同TRPO一样：
- 把目标在$\theta=\theta_k^{\,i_m}$​​ 处做**一阶近似**，梯度记为 $\mathbf g_k^{\,i_m}$；
- 把期望 $KL$ 在该点做**二阶近似**，Hessian 即 Fisher 信息矩阵 $\mathbf H_k^{\,i_m}​​$。$H_{k}^{i_m} = \nabla_{\theta^{i_m}}^2 \mathbb{E}_{s \sim \rho_{\pi_{\theta_k}}} \left[ D_{KL}(\pi_{\theta^{i_m}_k}^{i_m}(\cdot|s), \pi_{\theta^{i_m}}^{i_m}(\cdot|s)) \right] \bigg|_{\theta^{i_m}=\theta_k^{i_m}}$

$$
θ_{k+1}^{i_m} = θ_k^{i_m} + α^j \sqrt{\frac{2δ}{g_k^{i_m} (H_k^{i_m})^{-1} g_k^{i_m}}} .
$$
最后一步是求$\mathbb{E}_{\mathbf{a}^{i1:m-1} \sim \pi_{\theta_k^{i1:m-1}}, a^{im} \sim \pi_{\theta^{im}}^{im}} \left[ A_{\pi_{\theta_k}}^{im}(s, \mathbf{a}^{i1:m-1}, a^{im}) \right],$之后没看懂总之
HAPPO的目标是最大化
$$\mathbb{E}_{s \sim \rho_{\pi_{\theta_k}}, \mathbf{a} \sim \pi_{\theta_k}} \left[ \min \left( \frac{\pi_{\theta^{i_m}}^{i_m}(a^{i_m}|s)}{\pi_{\theta_k^{i_m}}^{i_m}(a^{i_m}|s)} M^{i1:m}(s, \mathbf{a}), \text{clip}\left(\frac{\pi_{\theta^{i_m}}^{i_m}(a^{i_m}|s)}{\pi_{\theta_k^{i_m}}^{i_m}(a^{i_m}|s)}, 1 \pm \epsilon \right) M^{i1:m}(s, \mathbf{a}) \right) \right].$$


